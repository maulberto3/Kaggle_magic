{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda list statsmodels","metadata":{"execution":{"iopub.status.busy":"2022-01-02T05:39:27.773171Z","iopub.execute_input":"2022-01-02T05:39:27.773645Z","iopub.status.idle":"2022-01-02T05:39:49.825466Z","shell.execute_reply.started":"2022-01-02T05:39:27.773613Z","shell.execute_reply":"2022-01-02T05:39:49.824424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip uninstall statsmodels -y\n!pip install -U statsmodels","metadata":{"execution":{"iopub.status.busy":"2022-01-02T03:27:42.639945Z","iopub.execute_input":"2022-01-02T03:27:42.640289Z","iopub.status.idle":"2022-01-02T03:27:56.894319Z","shell.execute_reply.started":"2022-01-02T03:27:42.640244Z","shell.execute_reply":"2022-01-02T03:27:56.893269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","metadata":{"execution":{"iopub.status.busy":"2022-01-02T03:28:01.111349Z","iopub.execute_input":"2022-01-02T03:28:01.11256Z","iopub.status.idle":"2022-01-02T03:28:01.117856Z","shell.execute_reply.started":"2022-01-02T03:28:01.112485Z","shell.execute_reply":"2022-01-02T03:28:01.117038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\npath = Path().cwd().parent / 'input'\nfiles = list(path.rglob('*'))\nfiles","metadata":{"execution":{"iopub.status.busy":"2022-01-02T04:04:32.081947Z","iopub.execute_input":"2022-01-02T04:04:32.082233Z","iopub.status.idle":"2022-01-02T04:04:32.092222Z","shell.execute_reply.started":"2022-01-02T04:04:32.082204Z","shell.execute_reply":"2022-01-02T04:04:32.091565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(files[2])\ntrain['date'] = pd.to_datetime(train['date'], errors='coerce')\ntrain = train.set_index('date')\n\ntest = pd.read_csv(files[3])\ntest['date'] = pd.to_datetime(test['date'], errors='coerce')\ntest = test.set_index('date')\n\ntrain\ntest","metadata":{"execution":{"iopub.status.busy":"2022-01-02T04:06:23.725048Z","iopub.execute_input":"2022-01-02T04:06:23.725414Z","iopub.status.idle":"2022-01-02T04:06:23.813218Z","shell.execute_reply.started":"2022-01-02T04:06:23.725376Z","shell.execute_reply":"2022-01-02T04:06:23.812277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking before resample\n\ndef len_df(df_wide):\n    train_dfs = {}\n    test_dfs = {}\n    for cat in df_wide['country'].unique():\n        for cat_ in df_wide['store'].unique():\n            for cat__ in df_wide['product'].unique():\n                mask = (df_wide['country']==cat) & (df_wide['store']==cat_) & (df_wide['product']==cat__)\n                train_dfs[cat+cat_+cat__] = df_wide[mask]\n                \n                train_dfs[cat+cat_+cat__] = train_dfs[cat+cat_+cat__]\n                print(df_wide[mask].shape, end=' ')\n    return dfs\n\ntrain_dfs = len_df(train)\ntest_dfs = len_df(test)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T04:13:00.837092Z","iopub.execute_input":"2022-01-02T04:13:00.839694Z","iopub.status.idle":"2022-01-02T04:13:01.230787Z","shell.execute_reply.started":"2022-01-02T04:13:00.839648Z","shell.execute_reply":"2022-01-02T04:13:01.229846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking after resample\n\ntrain2 = train.groupby(['country', 'store', 'product']).resample('D').sum()\ntrain2 = train2.reset_index(level=[0,1,2])\n\ntest2 = test.groupby(['country', 'store', 'product']).resample('D').sum()\ntest2 = test2.reset_index(level=[0,1,2])\n\ntrain2_dfs = len_df(train)\ntest2_dfs = len_df(test)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T04:13:14.680603Z","iopub.execute_input":"2022-01-02T04:13:14.680882Z","iopub.status.idle":"2022-01-02T04:13:15.137671Z","shell.execute_reply.started":"2022-01-02T04:13:14.680852Z","shell.execute_reply":"2022-01-02T04:13:15.136727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exploring categoricals\n\nfor col in train2[['country', 'store', 'product']]:\n    f'{col}, {train2[col].unique()}'","metadata":{"execution":{"iopub.status.busy":"2022-01-02T04:14:50.998147Z","iopub.execute_input":"2022-01-02T04:14:50.998617Z","iopub.status.idle":"2022-01-02T04:14:51.019965Z","shell.execute_reply.started":"2022-01-02T04:14:50.998583Z","shell.execute_reply":"2022-01-02T04:14:51.018119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Simple Split\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-02T04:38:10.736667Z","iopub.execute_input":"2022-01-02T04:38:10.736987Z","iopub.status.idle":"2022-01-02T04:38:10.741757Z","shell.execute_reply.started":"2022-01-02T04:38:10.736953Z","shell.execute_reply":"2022-01-02T04:38:10.740482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from statsmodels.tsa.api import STLForecast\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfrom math import sin\nfrom random import random\nimport matplotlib.pyplot as plt\n\nnames = list(train2_dfs.keys())\nser = train2_dfs[names[0]]['num_sold']\nres = STLForecast(ser, ARIMA, seasonal=23, robust=True).fit()\n#pred = res.get_prediction()\nforec = res.forecast(365)\n\nfig, ax = plt.subplots(figsize=(12,6))\n_ = ax.plot(ser)\n#_ = ax.plot(pred)\n_ = ax.plot(forec)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T04:36:49.273124Z","iopub.execute_input":"2022-01-02T04:36:49.273466Z","iopub.status.idle":"2022-01-02T04:36:50.033222Z","shell.execute_reply.started":"2022-01-02T04:36:49.273426Z","shell.execute_reply":"2022-01-02T04:36:50.032321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GA_Statsmodels():\n    def __init__(self, \n                 params, \n                 eval_func,\n                 eval_weights,\n                 #X_train,\n                 #X_test,\n                 #y_train,\n                 #y_test,\n                 #lr=0.0001,\n                 sel_tournsize=2, \n                 cx_uniform_prob=0.5, \n                 mut_shuffle_idx_prob=0.1, \n                 n_pop=35, \n                 n_gen=35, \n                 n_hof=5, \n                 cx_prob=0.5, \n                 mut_prob=0.1, \n                 n_jobs=1\n                ):\n        self.params = params\n        self.eval_func = eval_func\n        self.eval_weights = eval_weights\n        \n        #self.X_train = X_train\n        #self.X_test = X_test\n        #self.y_train = y_train\n        #self.y_test = y_test\n        #self.image_folder = image_folder\n        #self.batch_size = batch_size\n        #self.lr = lr\n        \n        self.sel_tournsize = sel_tournsize\n        self.cx_uniform_prob = cx_uniform_prob\n        self.mut_shuffle_idx_prob = mut_shuffle_idx_prob\n        self.n_pop = n_pop\n        self.n_gen = n_gen\n        self.n_hof = n_hof\n        self.cx_prob = cx_prob\n        self.mut_prob = mut_prob\n        \n        self.n_jobs = n_jobs\n\n        self._pad_params()\n        self._create_fitness_and_indiv()\n        self._register_indiv_and_pop_generators()\n        #self._register_eval_func()\n        #self._register_selection_crossover_mutation_methods()\n\n    def _pad_params(self):\n        \"\"\"Pad params for crossover shuffle idx method\"\"\"\n        assert isinstance(self.params, dict), 'Params must be a dict, i.e. estimator.get_params()'\n        params_count = {k: len(v) for k,v in self.params.items()}\n        max_length, max_key = -99, ''\n        for k, v in params_count.items():\n            if v <= max_length:\n                continue\n            else:\n                max_key = k\n                max_length = v\n        assert isinstance(max_length, int), 'The max length between all params must be an int'\n        # cycle through params for max length param, otherwise infinite cycle\n        values_padded = (cycle(v) if k!=max_key else v for k,v in self.params.items())\n        values_padded = zip(*values_padded)  # ('a', 1, 14), ('b', 2, 16), ('c', 3, 16) ...\n        values_padded = zip(*values_padded)  # ('a', 'b', 'c'), (1, 2, 3), (14, 15, 16)...\n        padded_params = {}\n        for k, v in zip(self.params, values_padded):\n            padded_params[k] = v\n        self.padded_params = padded_params\n        print('Params padded')\n\n    def _create_fitness_and_indiv(self):\n        \"\"\"Create GA individual and fitness entities (classes)\"\"\"\n        ga_cr.create('Fitness', ga_b.Fitness, weights=self.eval_weights)\n        ga_cr.create('Individual', list, fitness=ga_cr.Fitness)\n        print('GA entities created')\n\n    def _gen_params_to_ga(self):\n        \"\"\"Generate index for each param for individual\"\"\"\n        max_dict = len(self.padded_params)\n        max_length = len(list(self.padded_params.values())[0])\n        idxs = [randint(0, max_length-1) for _ in range(max_dict)]\n        return idxs\n    \n    def _register_indiv_and_pop_generators(self):\n        \"\"\"Register GA individual and population generators\"\"\"\n        self.tb = ga_b.Toolbox()\n\n        if self.n_jobs > 1:\n            from multiprocessing import Pool\n            pool = Pool()\n            self.tb.register(\"map\", pool.map)\n\n        self.tb.register(\"individual\", ga_t.initIterate, ga_cr.Individual, self._gen_params_to_ga)\n        #print('indiv', self.tb.individual())\n        self.tb.register(\"population\", ga_t.initRepeat, list, self.tb.individual)\n        #print('population', self.tb.population(n=2))\n        print('GA entities\\' methods registered')\n        \n    def _register_eval_func(self):\n        \"\"\"Set GA evaluate individual function\"\"\"\n        self.tb.register(\"evaluate\",\n                        self.eval_func,\n                        padded_params=self.padded_params,\n                        #image_folder=self.image_folder,\n                        #X_train=self.X_train,\n                        #X_test=self.X_test, \n                        #y_train=self.y_train, \n                        #y_test=self.y_test,\n                        #batch_size=self.batch_size,\n                        #lr=self.lr\n                        )\n        #print(list(self.tb.evaluate(indiv) for indiv in self.tb.population(3)))\n        print('GA eval function registered')\n    \n    def _register_selection_crossover_mutation_methods(self):\n        self.tb.register(\"select\", ga_t.selTournament, tournsize=self.sel_tournsize)\n        self.tb.register(\"mate\", ga_t.cxUniform, indpb=self.cx_uniform_prob)\n        self.tb.register(\"mutate\", ga_t.mutShuffleIndexes, indpb=self.mut_shuffle_idx_prob)\n        print('GA sel-cx-mut methods registered')\n        \n    def run_ga_search(self):\n        \"\"\"GA Search\"\"\"\n        pop = self.tb.population(n=self.n_pop)\n        hof = ga_t.HallOfFame(self.n_hof)\n\n        # Stats stdout\n        #stats = ga_t.Statistics(lambda ind: ind.fitness.values )\n        stats1 = ga_t.Statistics(lambda ind: ind.fitness.values[0] )\n        stats2 = ga_t.Statistics(lambda ind: ind.fitness.values[1] )\n        stats3 = ga_t.Statistics(lambda ind: ind.fitness.values[2] )\n        stats = ga_t.MultiStatistics(accuracy=stats1, risk=stats2, complexity=stats3)\n        stats.register(\"avg\", np.mean)\n        #stats.register(\"std\", np.std)\n        #stats.register(\"min\", np.min)\n        #stats.register(\"max\", np.max)\n\n        # History\n        #hist = tools.History()\n        #toolbox.decorate(\"select\", hist.decorator)\n        #tb.decorate(\"mate\", hist.decorator)\n        #tb.decorate(\"mutate\", hist.decorator)\n        #hist.update(pop)\n\n        # GA Run\n        pop, log = ga_algo.eaSimple(pop, self.tb, cxpb=self.cx_prob, \n                                    mutpb=self.mut_prob, ngen=self.n_gen, \n                                    stats=stats, halloffame=hof, verbose=True)\n        \n        # Convert back params\n        hof_ = {}\n        for i in range(self.n_hof):\n            hof_['hof_' + str(i)] = self._ga_to_params(hof[i])\n\n        return pop, log, hof_\n    \n    def _ga_to_params(self, idx_params):\n        \"\"\"Convert back idx to params\"\"\"\n        res = {}\n        for (k,v), idx in zip(self.padded_params.items(), idx_params):\n            res[k] = v[idx]\n        return res","metadata":{"execution":{"iopub.status.busy":"2022-01-02T03:52:23.049298Z","iopub.execute_input":"2022-01-02T03:52:23.049652Z","iopub.status.idle":"2022-01-02T03:52:23.081363Z","shell.execute_reply.started":"2022-01-02T03:52:23.04962Z","shell.execute_reply":"2022-01-02T03:52:23.080716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net_params = {\n    'ar': [0,1,2,3,4],\n    'd': [0,1,2,3],\n    'ma': [0,1,2,3,4],\n    \n    \n}\n\ndef net_eval_indiv(individual, padded_params):\n    \"\"\"Evaluate individual's genes (estimator's params)\"\"\"\n    # Params\n    indiv_params = {k : list(v)[idx] for (k,v), idx in zip(padded_params.items(), individual)}\n    \n    # Data\n\n    # Model\n    try:\n        net = NET(batch_size=learning_params['batch_size'], params=net_params)\n        net = net.to(device)\n\n        # Optimizer\n        optimizer = Adam(net.parameters(), lr=learning_params['lr'])\n        criterion = nn.NLLLoss()\n\n        # Train\n        #train_ds = DS(X_train, y_train)  # TODO refactor out\n        train_dl = DataLoader(image_folder,\n                            batch_size=learning_params['batch_size'],\n                            shuffle=True,\n                            num_workers=2,\n                            drop_last=True)\n\n        for epoch in range(1):\n            #running_loss = []\n            train_correct = 0\n            train_total = 0\n            for i, (inputs, labels) in enumerate(train_dl):\n                if i <= 40:\n                    inputs = inputs.to(device)\n                    labels = labels.to(device)\n                    outputs = net(inputs)\n                    outputs = pt_log(outputs)\n\n                    optimizer.zero_grad()\n                    loss = criterion(outputs, labels).mean()\n                    loss.backward()\n                    optimizer.step()\n\n                    # print statistics\n                    #running_loss.append(loss.item())\n                    _, predicted = pt_max(outputs.data, 1)\n                    train_total += labels.size(0)\n                    train_correct += (predicted == labels).sum().item()\n                    train_accuracy = train_correct / train_total * 100\n                    #print(f'TRAIN {train_accuracy:^5.2f} %', end=' ')\n                else:\n                    break\n    except BaseException as e:\n        print(e)\n        return (0.01, 10, 1000000,)\n        \n    # Eval\n    \"\"\"with no_grad():\n        net = net.eval()\n        test_ds = DS(X_test, y_test)  # TODO refactor out\n        test_dl = DataLoader(test_ds,\n                            batch_size=batch_size,\n                            shuffle=True,\n                            drop_last=True)\n        #running_loss = []\n        test_correct = 0\n        test_total = 0\n        for i, (inputs, labels) in enumerate(test_dl):\n            if i <= 100:\n                #inputs = inputs.cuda()\n                #labels = labels.cuda()\n                outputs = net(inputs)\n\n                # print statistics\n                #running_loss.append(loss.item())\n                _, predicted = pt_max(outputs.data, 1)\n                test_total += labels.size(0)\n                test_correct += (predicted == labels).sum().item()\n                test_accuracy = test_correct / test_total * 100\n            else:\n                break\n        #print(f'TEST {test_accuracy:^5.2f} %')\"\"\"\n        \n    # Risk\n    inputs = inputs.to(device)\n    risk = mean(prod(net(inputs)*10, dim=1))\n    if isnan(risk):\n        risk = 10\n    else:\n        risk = float(risk)\n        \n    # Complexity\n    compl = net.count_weights_biases()\n\n    return (train_accuracy, risk, compl,)\n\nnet_weights = (1, -1, -1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda list statsmodels","metadata":{"execution":{"iopub.status.busy":"2022-01-02T03:26:13.777952Z","iopub.execute_input":"2022-01-02T03:26:13.778254Z","iopub.status.idle":"2022-01-02T03:26:35.869441Z","shell.execute_reply.started":"2022-01-02T03:26:13.778225Z","shell.execute_reply":"2022-01-02T03:26:35.868453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-01-02T03:23:15.168712Z","iopub.execute_input":"2022-01-02T03:23:15.169108Z","iopub.status.idle":"2022-01-02T03:23:15.232185Z","shell.execute_reply.started":"2022-01-02T03:23:15.169057Z","shell.execute_reply":"2022-01-02T03:23:15.231383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if False:\n    res = ARIMA(temp, (2,2,0)).fit(disp=0)\n    forec_periods = 24\n    forec, stderr, conf_int = res.forecast(forec_periods)\n    forec = pd.Series(forec, index=[temp.index.max()+pd.DateOffset(months=i+1) for i in range(forec_periods)])\n    forec\n\n    fig, ax = plt.subplots(figsize=(12,6))\n    _ = ax.plot(temp)\n    _ = ax.plot(forec)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}