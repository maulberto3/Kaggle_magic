{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-26T17:49:36.790705Z","iopub.execute_input":"2021-12-26T17:49:36.791082Z","iopub.status.idle":"2021-12-26T17:49:36.821415Z","shell.execute_reply.started":"2021-12-26T17:49:36.790969Z","shell.execute_reply":"2021-12-26T17:49:36.820732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.set_printoptions(suppress=True)\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:49:43.200616Z","iopub.execute_input":"2021-12-27T05:49:43.201072Z","iopub.status.idle":"2021-12-27T05:49:43.209012Z","shell.execute_reply.started":"2021-12-27T05:49:43.201012Z","shell.execute_reply":"2021-12-27T05:49:43.208073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Patch Xeon Intel OneAPI Scikit accelerator\n!pip install scikit-learn-intelex\nfrom sklearnex import patch_sklearn\npatch_sklearn()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:52:04.179991Z","iopub.execute_input":"2021-12-27T05:52:04.180429Z","iopub.status.idle":"2021-12-27T05:52:12.92507Z","shell.execute_reply.started":"2021-12-27T05:52:04.180343Z","shell.execute_reply":"2021-12-27T05:52:12.923365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data\nfrom sklearn.datasets import make_classification\nX, y = make_classification(1000, 100, n_redundant=0, n_repeated=0, scale=0.01, flip_y=0.1)\n#X.shape, y.shape\n\n# Data Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, )\n#X_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n# Data Prepr\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\npip = make_pipeline(RobustScaler())\nX_train_transf = pip.fit_transform(X_train)\nX_test_transf = pip.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:52:15.241959Z","iopub.execute_input":"2021-12-27T05:52:15.242282Z","iopub.status.idle":"2021-12-27T05:52:15.296906Z","shell.execute_reply.started":"2021-12-27T05:52:15.24224Z","shell.execute_reply":"2021-12-27T05:52:15.296003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from random import randint\nfrom itertools import cycle\nfrom time import monotonic\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom deap import creator as ga_cr, base as ga_b, algorithms as ga_algo, tools as ga_t\n\nclass GA_Scikit():\n    def __init__(self, estimator, params, eval_func, eval_weights, X_train, \n                 X_test, y_train, y_test, score, sel_tournsize=2, cx_uniform_prob=0.5, \n                 mut_shuffle_idx_prob=0.1, n_pop=25, n_gen=10, n_hof=3, cx_prob=0.5, \n                 mut_prob=0.1, n_jobs=4):\n        self.est = estimator\n        self.params = params\n        self.eval_func = eval_func\n        self.eval_weights = eval_weights\n        self.X_train = X_train\n        self.X_test = X_test\n        self.y_train = y_train\n        self.y_test = y_test\n        self.score = score\n        self.sel_tournsize = sel_tournsize\n        self.cx_uniform_prob = cx_uniform_prob\n        self.mut_shuffle_idx_prob = mut_shuffle_idx_prob\n        self.n_pop = n_pop\n        self.n_gen = n_gen\n        self.n_hof = n_hof\n        self.cx_prob = cx_prob\n        self.mut_prob = mut_prob\n        self.n_jobs = n_jobs\n        \n        self._pad_params()\n        self._create_fitness_and_indiv()\n        self._register_indiv_and_pop_generators()\n        self._register_eval_func()\n        self._register_selection_crossover_mutation_methods()\n        \n    def _pad_params(self):\n        \"\"\"Pad params for crossover shuffle idx method\"\"\"\n        assert isinstance(self.params, dict), 'Params must be a dict, i.e. estimator.get_params()'\n        params_count = {k: len(v) for k,v in self.params.items()}\n        max_length, max_key = -99, ''\n        for k, v in params_count.items():\n            if v <= max_length:\n                continue\n            else:\n                max_key = k\n                max_length = v\n        assert isinstance(max_length, int), 'The max length between all params must be an int'\n        # cycle through params for max length param, otherwise infinite cycle\n        values_padded = (cycle(v) if k!=max_key else v for k,v in self.params.items())\n        values_padded = zip(*values_padded)  # ('a', 1, 14), ('b', 2, 16), ('c', 3, 16) ...\n        values_padded = zip(*values_padded)  # ('a', 'b', 'c'), (1, 2, 3), (14, 15, 16)...\n        padded_params = {}\n        for k, v in zip(self.params, values_padded):\n            padded_params[k] = v\n        self.padded_params = padded_params\n        \n    def _create_fitness_and_indiv(self):\n        \"\"\"Create GA individual and fitness entities (classes)\"\"\"\n        ga_cr.create('Fitness', ga_b.Fitness, weights=self.eval_weights)\n        ga_cr.create('Individual', list, fitness=ga_cr.Fitness)\n\n    def _gen_params_to_ga(self):\n        \"\"\"Generate index for each param for individual\"\"\"\n        max_dict = len(self.padded_params)\n        max_length = len(list(self.padded_params.values())[0])\n        idxs = [randint(0, max_length-1) for _ in range(max_dict)]\n        return idxs\n            \n    def _register_indiv_and_pop_generators(self):\n        \"\"\"Register GA individual and population generators\"\"\"\n        self.tb = ga_b.Toolbox()\n\n        if self.n_jobs > 1:\n            from multiprocessing import Pool\n            pool = Pool()\n            self.tb.register(\"map\", pool.map)\n\n        self.tb.register(\"individual\", ga_t.initIterate, ga_cr.Individual, self._gen_params_to_ga)\n        self.tb.register(\"population\", ga_t.initRepeat, list, self.tb.individual)\n    \n    def _register_eval_func(self):\n        \"\"\"Set GA evaluate individual function\"\"\"\n        self.tb.register(\"evaluate\",\n                        self.eval_func,\n                        padded_params=self.padded_params,\n                        est=self.est,\n                        X_train=self.X_train,\n                        X_test=self.X_test, \n                        y_train=self.y_train, \n                        y_test=self.y_test,\n                        score=self.score)\n        \n    def _register_selection_crossover_mutation_methods(self):\n        self.tb.register(\"select\", ga_t.selTournament, tournsize=self.sel_tournsize)\n        self.tb.register(\"mate\", ga_t.cxUniform, indpb=self.cx_uniform_prob)\n        self.tb.register(\"mutate\", ga_t.mutShuffleIndexes, indpb=self.mut_shuffle_idx_prob)\n        \n    def run_ga_search(self):\n        pop = self.tb.population(n=self.n_pop)\n        hof = ga_t.HallOfFame(self.n_hof)\n\n        # Stats stdout\n        stats1 = ga_t.Statistics(lambda ind: ind.fitness.values[0] )\n        stats2 = ga_t.Statistics(lambda ind: ind.fitness.values[1] )\n        stats3 = ga_t.Statistics(lambda ind: ind.fitness.values[2] )\n        stats = ga_t.MultiStatistics(score=stats1, duration=stats2, risk=stats3)\n        stats.register(\"avg\", np.mean)\n        #stats.register(\"std\", np.std)\n        #stats.register(\"min\", np.min)\n        #stats.register(\"max\", np.max)\n\n        # History\n        #hist = tools.History()\n        #toolbox.decorate(\"select\", hist.decorator)\n        #tb.decorate(\"mate\", hist.decorator)\n        #tb.decorate(\"mutate\", hist.decorator)\n        #hist.update(pop)\n\n        # GA Run\n        pop, log = ga_algo.eaSimple(pop, self.tb, cxpb=self.cx_prob, \n                                    mutpb=self.mut_prob, ngen=self.n_gen, \n                                    stats=stats, halloffame=hof, verbose=True)\n        \n        # Convert back params\n        hof_ = {}\n        for i in range(self.n_hof):\n            hof_['hof_' + str(i)] = self._ga_to_params(hof[i])\n\n        return pop, log, hof_\n    \n    def _ga_to_params(self, idx_params):\n        res = {}\n        for (k,v), idx in zip(self.padded_params.items(), idx_params):\n            res[k] = v[idx]\n        return res","metadata":{"execution":{"iopub.status.busy":"2021-12-28T06:20:38.182039Z","iopub.execute_input":"2021-12-28T06:20:38.1824Z","iopub.status.idle":"2021-12-28T06:20:39.488928Z","shell.execute_reply.started":"2021-12-28T06:20:38.182299Z","shell.execute_reply":"2021-12-28T06:20:39.487913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Estimator, params and requirements\n\nrf = RandomForestClassifier()\n\nrf_params = {\n            'class_weight': ['balanced', 'balanced_subsample'],\n            'bootstrap': [False, True],\n            'n_estimators': np.linspace(1, 100, 100).astype(int),\n            'max_depth': np.linspace(1, 100, 100).astype(int),\n            'criterion': ['gini', 'entropy'],\n            'max_features': np.linspace(.01, .99, 100),\n            'max_samples': np.linspace(.01, .99, 100),\n             }\n\ndef rf_eval_indiv(individual, padded_params, est, X_train, X_test, y_train, y_test, score):\n    \"\"\"Evaluate individual's genes (estimator's params)\"\"\"\n    ### => Result tuple must match weights in GA\n    indiv_params = {k : list(v)[idx] for (k,v), idx in zip(padded_params.items(), individual)}\n    est.set_params(**{**indiv_params, **{'n_jobs': 1} })  # seems can't parallelize this simultaneous with GA\n    est.fit(X_train, y_train)\n    start = monotonic()\n    pred = est.predict(X_test)\n    obj2 = monotonic() - start\n    obj1 = score(y_test, pred)\n    pred_proba = est.predict_proba(X_test)\n    obj3 = float(np.quantile(pred_proba.prod(axis=1), 0.5))\n    return (obj1, obj2, obj3)\n        \neval_weights = (1, -1, -1)\n# sel_tournsize, cx_uniform_prob, mut_shuffle_idx_prob = 2, 0.5, 0.1\n# n_pop, n_gen, n_hof = 5, 5, 3\n# cx_prob, mut_prob = 0.5, 0.1\n\nga_params = GA_Scikit(rf, \n                  rf_params, \n                  rf_eval_indiv, \n                  eval_weights,\n                  X_train_transf,\n                  X_test_transf,\n                  y_train,\n                  y_test,\n                  accuracy_score,\n                  #sel_tournsize,\n                  #cx_uniform_prob,\n                  #mut_shuffle_idx_prob,\n                  #n_pop,\n                  #n_gen,\n                  #n_hof,\n                  #cx_prob,\n                  #mut_prob\n                  )\npop, log, hof = ga_params.run_ga_search()\nhof","metadata":{"execution":{"iopub.status.busy":"2021-12-27T06:10:13.126726Z","iopub.execute_input":"2021-12-27T06:10:13.128022Z","iopub.status.idle":"2021-12-27T06:11:15.334175Z","shell.execute_reply.started":"2021-12-27T06:10:13.127915Z","shell.execute_reply":"2021-12-27T06:11:15.333374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\nad = AdaBoostClassifier()\n#ad.get_params()\n\nad_params = {\n    'algorithm': ['SAMME'],\n     'base_estimator': [DecisionTreeClassifier(), LogisticRegression()],\n     'learning_rate': np.linspace(0.001, 0.1, 100),\n     'n_estimators': np.linspace(1, 100, 100).astype(int),\n}\n\ndef ad_eval_indiv(individual, padded_params, est, X_train, X_test, y_train, y_test, score):\n    \"\"\"Evaluate individual's genes (estimator's params)\"\"\"\n    ### => Result tuple must match weights in GA\n    indiv_params = {k : list(v)[idx] for (k,v), idx in zip(padded_params.items(), individual)}\n    #est.set_params(**{**indiv_params, **{'n_jobs': 1} })  # seems can't parallelize this simultaneous with GA\n    est.set_params(**indiv_params)  # seems can't parallelize this simultaneous with GA\n    est.fit(X_train, y_train)\n    start = monotonic()\n    pred = est.predict(X_test)\n    obj2 = monotonic() - start\n    obj1 = score(y_test, pred)\n    pred_proba = est.predict_proba(X_test)\n    obj3 = float(np.quantile(pred_proba.prod(axis=1), 0.5))\n    return (obj1, obj2, obj3)\n        \neval_weights = (1, -1, -1)\n# sel_tournsize, cx_uniform_prob, mut_shuffle_idx_prob = 2, 0.5, 0.1\n# n_pop, n_gen, n_hof = 5, 5, 3\n# cx_prob, mut_prob = 0.5, 0.1\n\nga_params = GA_Scikit(ad, \n                  ad_params, \n                  ad_eval_indiv, \n                  eval_weights,\n                  X_train_transf,\n                  X_test_transf,\n                  y_train,\n                  y_test,\n                  accuracy_score,\n                  #sel_tournsize,\n                  #cx_uniform_prob,\n                  #mut_shuffle_idx_prob,\n                  #n_pop,\n                  #n_gen,\n                  #n_hof,\n                  #cx_prob,\n                  #mut_prob\n                  )\npop, log, hof = ga_params.run_ga_search()\nhof","metadata":{"execution":{"iopub.status.busy":"2021-12-27T06:08:26.973957Z","iopub.execute_input":"2021-12-27T06:08:26.974332Z","iopub.status.idle":"2021-12-27T06:09:20.112752Z","shell.execute_reply.started":"2021-12-27T06:08:26.974278Z","shell.execute_reply":"2021-12-27T06:09:20.111992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n\nxt = ExtraTreesClassifier()\n#xt.get_params()\n\nxt_params = {\n             'bootstrap': [False, True],\n             'ccp_alpha': np.linspace(0, 0.1, 100),\n             'class_weight': ['balanced', 'balanced_subsample'],\n             'criterion': ['gini', 'entropy'],\n             'max_depth': np.linspace(1, 100, 100).astype(int),\n             'max_features': np.linspace(0.01, 0.99, 100),\n             'max_leaf_nodes': np.linspace(1, 100, 100).astype(int),\n             'max_samples': np.linspace(0.01, 0.99, 100),\n             'min_impurity_decrease': np.linspace(0, 0.1, 100),\n             'min_samples_leaf': np.linspace(1, 100, 100).astype(int),\n             'min_samples_split': np.linspace(1, 100, 100).astype(int),\n             'n_estimators': np.linspace(1, 100, 100).astype(int),\n             }\n\ndef xt_eval_indiv(individual, padded_params, est, X_train, X_test, y_train, y_test, score):\n    \"\"\"Evaluate individual's genes (estimator's params)\"\"\"\n    ### => Result tuple must match weights in GA\n    indiv_params = {k : list(v)[idx] for (k,v), idx in zip(padded_params.items(), individual)}\n    est.set_params(**{**indiv_params, **{'n_jobs': 1} })  # seems can't parallelize this simultaneous with GA\n    est.fit(X_train, y_train)\n    start = monotonic()\n    pred = est.predict(X_test)\n    obj2 = monotonic() - start\n    obj1 = score(y_test, pred)\n    pred_proba = est.predict_proba(X_test)\n    obj3 = float(np.quantile(pred_proba.prod(axis=1), 0.5))\n    return (obj1, obj2, obj3)\n        \neval_weights = (1, -1, -1)\n# sel_tournsize, cx_uniform_prob, mut_shuffle_idx_prob = 2, 0.5, 0.1\n# n_pop, n_gen, n_hof = 5, 5, 3\n# cx_prob, mut_prob = 0.5, 0.1\n\nga_params = GA_Scikit(xt, \n                  xt_params, \n                  xt_eval_indiv, \n                  eval_weights,\n                  X_train_transf,\n                  X_test_transf,\n                  y_train,\n                  y_test,\n                  accuracy_score,\n                  #sel_tournsize,\n                  #cx_uniform_prob,\n                  #mut_shuffle_idx_prob,\n                  n_pop=50,\n                  n_gen=15,\n                  #n_hof,\n                  #cx_prob,\n                  #mut_prob\n                  )\npop, log, hof = ga_params.run_ga_search()\nhof","metadata":{"execution":{"iopub.status.busy":"2021-12-27T06:19:55.708653Z","iopub.execute_input":"2021-12-27T06:19:55.709085Z","iopub.status.idle":"2021-12-27T06:20:34.365981Z","shell.execute_reply.started":"2021-12-27T06:19:55.709024Z","shell.execute_reply":"2021-12-27T06:20:34.364928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xt_params_best = {'bootstrap': False,\n  'ccp_alpha': 0.030303030303030304,\n  'class_weight': 'balanced_subsample',\n  'criterion': 'entropy',\n  'max_depth': 27,\n  'max_features': 0.9504040404040404,\n  'max_leaf_nodes': 65,\n  'max_samples': 0.7722222222222221,\n  'min_impurity_decrease': 0.012121212121212121,\n  'min_samples_leaf': 15,\n  'min_samples_split': 65,\n  'n_estimators': 58}\nxt.set_params(**xt_params_best)\nxt.fit(X_train_transf, y_train)\nxt.predict_proba(X_test_transf).prod(axis=1).mean()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T06:25:55.522991Z","iopub.execute_input":"2021-12-27T06:25:55.523322Z","iopub.status.idle":"2021-12-27T06:25:55.738207Z","shell.execute_reply.started":"2021-12-27T06:25:55.523281Z","shell.execute_reply":"2021-12-27T06:25:55.737146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_params_best = {'class_weight': 'balanced_subsample',\n  'bootstrap': True,\n  'n_estimators': 17,\n  'max_depth': 4,\n  'criterion': 'gini',\n  'max_features': 0.6039393939393939,\n  'max_samples': 0.8712121212121212}\nrf.set_params(**rf_params_best)\nrf.fit(X_train_transf, y_train)\nrf.predict_proba(X_test_transf).prod(axis=1).mean()","metadata":{"execution":{"iopub.status.busy":"2021-12-27T06:25:50.145595Z","iopub.execute_input":"2021-12-27T06:25:50.145886Z","iopub.status.idle":"2021-12-27T06:25:50.201424Z","shell.execute_reply.started":"2021-12-27T06:25:50.145852Z","shell.execute_reply":"2021-12-27T06:25:50.200209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# PREVIOUS WORK","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Long version, simple OneMax problem\n\n# Class factory \ncreator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\ncreator.create(\"Individual\", list, fitness=creator.FitnessMax)\n\n# everything else is stored inside the toolbox\ntoolbox = base.Toolbox()\ntoolbox.register(\"attr_bool\", randint, 0, 1)\ntoolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, 100)\ntoolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n\ndef evalOneMax(individual):\n    return sum(individual),\n\ntoolbox.register(\"evaluate\", evalOneMax)\ntoolbox.register(\"select\", tools.selTournament, tournsize=3)\ntoolbox.register(\"mate\", tools.cxTwoPoint)\ntoolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n\ndef main():\n    pop = toolbox.population(n=300)\n    \n    fitnesses = list(map(toolbox.evaluate, pop))\n    for ind, fit in zip(pop, fitnesses):\n        ind.fitness.values = fit\n        \n    crossover_prob, mutate_prob = 0.5, 0.2\n    \n    fits = [ind.fitness.values[0] for ind in pop]\n    \n    g = 1\n    while max(fits) < 100 and g < 1000:\n        print(f\"Gen {g:^3}:\", end=' ')\n        \n        offspring = toolbox.select(pop, len(pop))\n        offspring = list(map(toolbox.clone, offspring))\n\n        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n            if random() < crossover_prob:\n                toolbox.mate(child1, child2)\n                del child1.fitness.values\n                del child2.fitness.values\n\n        for mutant in offspring:\n            if random() < mutate_prob:\n                toolbox.mutate(mutant)\n                del mutant.fitness.values\n                \n        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n        fitnesses = map(toolbox.evaluate, invalid_ind)\n        for ind, fit in zip(invalid_ind, fitnesses):\n            ind.fitness.values = fit\n            \n        pop[:] = offspring\n        \n        fits = [ind.fitness.values[0] for ind in pop]\n        \n        length = len(pop)\n        mean = sum(fits) / length\n        sum2 = sum(x*x for x in fits)\n        std = abs(sum2 / length - mean**2)**0.5\n        \n        print(f\"Min {min(fits):.2f} |\", end=' ')\n        print(f\"Max {max(fits):.2f} |\", end=' ')\n        print(f\"Avg {mean:.2f} |\", end=' ')\n        print(f\"Std {std:.2f}\")\n                \n        g += 1\n        \nmain()        ","metadata":{"execution":{"iopub.status.busy":"2021-12-23T22:57:33.113174Z","iopub.execute_input":"2021-12-23T22:57:33.113857Z","iopub.status.idle":"2021-12-23T22:57:36.159655Z","shell.execute_reply.started":"2021-12-23T22:57:33.113798Z","shell.execute_reply":"2021-12-23T22:57:36.158345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Short version, simple OneMax problem and similar\n\ncreator.create(\"FitnessMax\", base.Fitness, weights=(-1.0,))\ncreator.create(\"Individual\", list, fitness=creator.FitnessMax)\n\ntoolbox = base.Toolbox()\ntoolbox.register(\"attr_bool\", randint, 0, 1)\ntoolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, 1000)\ntoolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n\ndef evalOneMax(individual):\n    \"\"\"MSE-ish\"\"\"\n    res = sum(individual)\n    #if res < 0:\n      #  return (500,)\n    #if any((gene < 0 for gene in individual)):\n      #  return (500,)\n    #else:\n    return sum([gene**2 for gene in individual])/len(individual),\n\ntoolbox.register(\"evaluate\", evalOneMax)\ntoolbox.register(\"select\", tools.selTournament, tournsize=2)\ntoolbox.register(\"mate\", tools.cxUniform, indpb=0.5)\ntoolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.1)\n\ndef main():\n    pop = toolbox.population(n=500)\n    hof = tools.HallOfFame(1)\n    \n    stats = tools.Statistics(lambda ind: ind.fitness.values)\n    stats.register(\"avg\", np.mean)\n    stats.register(\"std\", np.std)\n    stats.register(\"min\", np.min)\n    stats.register(\"max\", np.max)\n    \n    pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0.5, mutpb=0.01, ngen=200, \n                                   stats=stats, halloffame=hof, verbose=True)\n    \n    return pop, hof\n    \n#pop, hof = main()","metadata":{"execution":{"iopub.status.busy":"2021-12-26T17:50:30.612907Z","iopub.execute_input":"2021-12-26T17:50:30.613179Z","iopub.status.idle":"2021-12-26T17:50:30.691149Z","shell.execute_reply.started":"2021-12-26T17:50:30.613151Z","shell.execute_reply":"2021-12-26T17:50:30.689839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"creator.create('Fitness', base.Fitness, weights=(1,))\ncreator.create('Individual', list, fitness=creator.Fitness)\n\ntb = base.Toolbox()\ntb.register(\"attr_bool\", randint, 0, 3)\ntb.register(\"individual\", tools.initRepeat, creator.Individual, tb.attr_bool, 5)\ntb.register(\"population\", tools.initRepeat, list, tb.individual)\n\ntb.population(5)","metadata":{"execution":{"iopub.status.busy":"2021-12-26T03:13:35.498677Z","iopub.execute_input":"2021-12-26T03:13:35.499233Z","iopub.status.idle":"2021-12-26T03:13:35.514074Z","shell.execute_reply.started":"2021-12-26T03:13:35.499194Z","shell.execute_reply":"2021-12-26T03:13:35.513174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some tests\n\nfrom itertools import combinations\n\ncreator.create('Fitness', base.Fitness, weights=(1,))\ncreator.create('Individual', list, fitness=creator.Fitness)\n\ndef some_gen():\n    nums = np.array(range(50))\n    idx = np.random.randint(0, 50, size=5)\n    return nums[idx]\n\ntb = base.Toolbox()\ntb.register(\"individual\", tools.initIterate, creator.Individual, some_gen)\ntb.register(\"population\", tools.initRepeat, list, tb.individual)\n\npop = tb.population(5)\n\ntb.register(\"mate\", tools.cxUniform, indpb=0.20)\npop[0]\npop[1]\ntb.mate(pop[0], pop[1])\n\ntb.register(\"mutate\", tools.mutShuffleIndexes, indpb=0.1)\ntb.mutate(pop[0])","metadata":{"execution":{"iopub.status.busy":"2021-12-26T04:41:58.749725Z","iopub.execute_input":"2021-12-26T04:41:58.750089Z","iopub.status.idle":"2021-12-26T04:41:58.775833Z","shell.execute_reply.started":"2021-12-26T04:41:58.750052Z","shell.execute_reply":"2021-12-26T04:41:58.775054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install scikit-learn-intelex","metadata":{"execution":{"iopub.status.busy":"2021-12-26T17:50:45.972275Z","iopub.execute_input":"2021-12-26T17:50:45.972754Z","iopub.status.idle":"2021-12-26T17:51:24.105566Z","shell.execute_reply.started":"2021-12-26T17:50:45.972721Z","shell.execute_reply":"2021-12-26T17:51:24.104791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Patch Xeon Intel OneAPI Scikit accelerator\n!pip install scikit-learn-intelex\nfrom sklearnex import patch_sklearn\npatch_sklearn()","metadata":{"execution":{"iopub.status.busy":"2021-12-26T17:51:38.046134Z","iopub.execute_input":"2021-12-26T17:51:38.046471Z","iopub.status.idle":"2021-12-26T17:51:39.423932Z","shell.execute_reply.started":"2021-12-26T17:51:38.046438Z","shell.execute_reply":"2021-12-26T17:51:39.423103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data\nfrom sklearn.datasets import make_classification\nX, y = make_classification(1000, 100, n_redundant=0, n_repeated=0, scale=0.01, flip_y=0.1)\n#X.shape, y.shape\n\n# Data Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, )\n#X_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n# Data Prepr\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\npip = make_pipeline(RobustScaler())\nX_train_transf = pip.fit_transform(X_train)\nX_test_transf = pip.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-27T05:45:08.544067Z","iopub.execute_input":"2021-12-27T05:45:08.545657Z","iopub.status.idle":"2021-12-27T05:45:08.720378Z","shell.execute_reply.started":"2021-12-27T05:45:08.545576Z","shell.execute_reply":"2021-12-27T05:45:08.718954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# My DEAP Scikit implementation\n# Trick Covnert params to n*n grid, feed idxs to GA\n# TODO code class object\n# TODO code cli possibly with click package and publish\n# TODO ...\n# Can be unique or multiobjective\n\nfrom random import randint\nfrom itertools import cycle, chain\nfrom collections import defaultdict\n\n# Estimator params\nrf_params = {\n            #'class_weight': ['balanced', 'balanced_subsample'],\n            #'bootstrap': [False, True],\n            'n_estimators': np.linspace(1,50).astype(int),\n            'max_depth': np.linspace(1,50).astype(int),\n             #'criterion': ['gini', 'entropy'],\n             'max_features': np.linspace(.01, .99),\n             'max_samples': np.linspace(.01, .99),\n             }\n\n# Pad estimator params\ndef _params_pad(params):\n    \"\"\"Pad params for crossover shuffle idx method\"\"\"\n    params_count = {k: len(v) for k,v in params.items()}\n    max_length, max_key = -99, ''\n    for k, v in params_count.items():\n        if v <= max_length:\n            continue\n        else:\n            max_key = k\n            max_length = v\n    assert isinstance(max_length, int), 'The max length between all params must be an int'\n    # cycle through params for max length param, otherwise infinite cycle\n    values_padded = (cycle(v) if k!=max_key else v for k,v in params.items())\n    values_padded = zip(*values_padded)  # ('a', 1, 14), ('b', 2, 16), ('c', 3, 16) ...\n    values_padded = zip(*values_padded)  # ('a', 'b', 'c'), (1, 2, 3), (14, 15, 16)...\n    padded_params = {}\n    for k, v in zip(params, values_padded):\n        padded_params[k] = v\n    return padded_params\n    \npadded_params = _params_pad(rf_params)\n#rf_params\n#padded_params\n\n# Params sampler for individual GA\ndef _gen_params(padded_params=padded_params):\n    \"\"\"Get some params idx for individual\"\"\"\n    max_dict = len(padded_params)\n    max_length = len(list(padded_params.values())[0])\n    return [randint(0, max_length-1) for _ in range(max_dict)]\n\n#some_params = _gen_params()\n#some_params\n\nfrom deap import creator, base, algorithms as ga_algo, tools\n\n# GA Fitness\ncreator.create('Fitness', base.Fitness, weights=(0.75,))\ncreator.create('Individual', list, fitness=creator.Fitness)\n\n# GA toolbox\ntb = base.Toolbox()\n\n# Multiprocessing for GA\n### TODO code for n_jobs\nfrom multiprocessing import Pool\npool = Pool()\ntb.register(\"map\", pool.map)\n\n# GA Individual and Pop\ntb.register(\"individual\", tools.initIterate, creator.Individual, _gen_params)\n#tb.individual()\ntb.register(\"population\", tools.initRepeat, list, tb.individual)\n#tb.population(3)\n\n# Specific fitness\ndef eval_indiv(individual, padded_params, est, X_train, X_test, y_train, y_test, score):\n    \"\"\"Evaluate individual's genes (estimator's params)\"\"\"\n    # convert indiv genes to estimator params\n    indiv_params = {k : list(v)[idx] for (k,v), idx in zip(padded_params.items(), individual)}\n    ### TODO code for n_jobs\n    est.set_params(**{**indiv_params, **{'n_jobs': -1} })\n    est.fit(X_train, y_train)\n    # ALWAYS return tuple\n    obj1 = score(y_test, est.predict(X_test))\n    obj2 = int(individual[0]) # n_estimators\n    return (obj1,)\n    \n# Scikit Estimator\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nfrom sklearn.metrics import accuracy_score\n\n# GA selection, crossover and mutation\ntb.register(\"evaluate\", \n            eval_indiv, \n            padded_params=padded_params, \n            est=rf, \n            X_train=X_train_transf, \n            X_test=X_test_transf, \n            y_train=y_train, \n            y_test=y_test, \n            score=accuracy_score)\ntb.register(\"select\", tools.selTournament, tournsize=2)\ntb.register(\"mate\", tools.cxUniform, indpb=0.5)\ntb.register(\"mutate\", tools.mutShuffleIndexes, indpb=0.1)\n\n#_eval(tb.individual(), padded_params, rf, X_train_transf, X_test_transf, y_train, y_test, accuracy_score)\n\ndef main(tb=tb, tools=tools, ga_algo=ga_algo, n_pop=50, n_gen=50):\n    pop = tb.population(n=n_pop)\n    hof = tools.HallOfFame(2)\n    \n    # Stats stdout\n    stats = tools.Statistics(lambda ind: ind.fitness.values )\n    #stats2 = tools.Statistics(lambda ind: ind.fitness.values[1] )\n    #stats = tools.MultiStatistics(acc=stats1, n_est=stats2)\n    stats.register(\"avg\", np.mean)\n    #stats.register(\"std\", np.std)\n    stats.register(\"min\", np.min)\n    stats.register(\"max\", np.max)\n    \n    # History\n    #hist = tools.History()\n    #toolbox.decorate(\"select\", hist.decorator)\n    #tb.decorate(\"mate\", hist.decorator)\n    #tb.decorate(\"mutate\", hist.decorator)\n    #hist.update(pop)\n    \n    # GA Run\n    pop, log = ga_algo.eaSimple(pop, tb, cxpb=0.5, mutpb=0.1, ngen=n_gen, stats=stats, halloffame=hof, verbose=True)\n    \n    return pop, log, hof\n    \npop, log, hof = main()\n\nf'Best Params {hof[0]}'\nf'Best Params 2nd {hof[1]}'","metadata":{"execution":{"iopub.status.busy":"2021-12-26T17:52:37.024815Z","iopub.execute_input":"2021-12-26T17:52:37.02547Z","iopub.status.idle":"2021-12-26T17:54:18.977073Z","shell.execute_reply.started":"2021-12-26T17:52:37.025404Z","shell.execute_reply":"2021-12-26T17:54:18.975995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same but multi-objecgive\n# Max accuracy score\n# min number of trees\n\nfrom random import randint\nfrom itertools import cycle, chain\nfrom collections import defaultdict\n\n# Estimator params\nrf_params = {\n            #'class_weight': ['balanced', 'balanced_subsample'],\n            #'bootstrap': [False, True],\n            'n_estimators': np.linspace(1,50).astype(int),\n            'max_depth': np.linspace(1,50).astype(int),\n             #'criterion': ['gini', 'entropy'],\n             'max_features': np.linspace(.01, .99),\n             'max_samples': np.linspace(.01, .99),\n             }\n\n# Pad estimator params\ndef _params_pad(params):\n    \"\"\"Pad params for crossover shuffle idx method\"\"\"\n    params_count = {k: len(v) for k,v in params.items()}\n    max_length, max_key = -99, ''\n    for k, v in params_count.items():\n        if v <= max_length:\n            continue\n        else:\n            max_key = k\n            max_length = v\n    assert isinstance(max_length, int), 'The max length between all params must be an int'\n    # cycle through params for max length param, otherwise infinite cycle\n    values_padded = (cycle(v) if k!=max_key else v for k,v in params.items())\n    values_padded = zip(*values_padded)  # ('a', 1, 14), ('b', 2, 16), ('c', 3, 16) ...\n    values_padded = zip(*values_padded)  # ('a', 'b', 'c'), (1, 2, 3), (14, 15, 16)...\n    padded_params = {}\n    for k, v in zip(params, values_padded):\n        padded_params[k] = v\n    return padded_params\n    \npadded_params = _params_pad(rf_params)\n#rf_params\n#padded_params\n\n# Params sampler for individual GA\ndef _gen_params(padded_params=padded_params):\n    \"\"\"Get some params idx for individual\"\"\"\n    max_dict = len(padded_params)\n    max_length = len(list(padded_params.values())[0])\n    return [randint(0, max_length-1) for _ in range(max_dict)]\n\n#some_params = _gen_params()\n#some_params\n\nfrom deap import creator, base, algorithms as ga_algo, tools, cma\n\n# GA Fitness\ncreator.create('Fitness', base.Fitness, weights=(1, -1))\ncreator.create('Individual', list, fitness=creator.Fitness)\n\n# GA toolbox\ntb = base.Toolbox()\n\n# Multiprocessing for GA\n### TODO code for n_jobs\nfrom multiprocessing import Pool\npool = Pool()\ntb.register(\"map\", pool.map)\n\n# GA Individual and Pop\ntb.register(\"individual\", tools.initIterate, creator.Individual, _gen_params)\n#tb.individual()\ntb.register(\"population\", tools.initRepeat, list, tb.individual)\n#tb.population(3)\n\n# Specific fitness\ndef eval_indiv(individual, padded_params, est, X_train, X_test, y_train, y_test, score):\n    \"\"\"Evaluate individual's genes (estimator's params)\"\"\"\n    # convert indiv genes to estimator params\n    indiv_params = {k : list(v)[idx] for (k,v), idx in zip(padded_params.items(), individual)}\n    ### TODO code for n_jobs\n    est.set_params(**{**indiv_params, **{'n_jobs': -1} })\n    est.fit(X_train, y_train)\n    # ALWAYS return tuple\n    obj1 = score(y_test, est.predict(X_test))\n    obj2 = int(individual[0]) # n_estimators\n    return (obj1, obj2)\n    \n# Scikit Estimator\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nfrom sklearn.metrics import accuracy_score\n\n# GA selection, crossover and mutation\ntb.register(\"evaluate\", \n            eval_indiv, \n            padded_params=padded_params, \n            est=rf, \n            X_train=X_train_transf, \n            X_test=X_test_transf, \n            y_train=y_train, \n            y_test=y_test, \n            score=accuracy_score)\ntb.register(\"select\", tools.selTournament, tournsize=2)\ntb.register(\"mate\", tools.cxUniform, indpb=0.5)\ntb.register(\"mutate\", tools.mutShuffleIndexes, indpb=0.1)\n\n#_eval(tb.individual(), padded_params, rf, X_train_transf, X_test_transf, y_train, y_test, accuracy_score)\n\ndef main(tb=tb, tools=tools, ga_algo=ga_algo, n_pop=50, n_gen=50):\n    pop = tb.population(n=n_pop)\n    hof = tools.HallOfFame(2)\n    \n    # Stats stdout\n    stats1 = tools.Statistics(lambda ind: ind.fitness.values[0] )\n    stats2 = tools.Statistics(lambda ind: ind.fitness.values[1] )\n    stats = tools.MultiStatistics(acc=stats1, n_est=stats2)\n    stats.register(\"avg\", np.mean)\n    #stats.register(\"std\", np.std)\n    stats.register(\"min\", np.min)\n    stats.register(\"max\", np.max)\n    \n    # History\n    #hist = tools.History()\n    #toolbox.decorate(\"select\", hist.decorator)\n    #tb.decorate(\"mate\", hist.decorator)\n    #tb.decorate(\"mutate\", hist.decorator)\n    #hist.update(pop)\n    \n    # GA Run\n    pop, log = ga_algo.eaSimple(pop, tb, cxpb=0.5, mutpb=0.1, ngen=n_gen, stats=stats, halloffame=hof, verbose=True)\n    \n    return pop, log, hof\n    \npop, log, hof = main()\n\nf'Best Params {hof[0]}'\nf'Best Params 2nd {hof[1]}'","metadata":{"execution":{"iopub.status.busy":"2021-12-26T17:56:40.205568Z","iopub.execute_input":"2021-12-26T17:56:40.205992Z","iopub.status.idle":"2021-12-26T17:58:21.459464Z","shell.execute_reply.started":"2021-12-26T17:56:40.205952Z","shell.execute_reply":"2021-12-26T17:58:21.45861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Same but multi-objecgive\n# Max accuracy score\n# min number of trees\n# Min risky predict probs\n\nfrom random import randint\nfrom itertools import cycle, chain\nfrom collections import defaultdict\n\n# Estimator params\nrf_params = {\n            #'class_weight': ['balanced', 'balanced_subsample'],\n            #'bootstrap': [False, True],\n            'n_estimators': np.linspace(1,50).astype(int),\n            'max_depth': np.linspace(1,50).astype(int),\n             #'criterion': ['gini', 'entropy'],\n             'max_features': np.linspace(.01, .99),\n             'max_samples': np.linspace(.01, .99),\n             }\n\n# Pad estimator params\ndef _params_pad(params):\n    \"\"\"Pad params for crossover shuffle idx method\"\"\"\n    params_count = {k: len(v) for k,v in params.items()}\n    max_length, max_key = -99, ''\n    for k, v in params_count.items():\n        if v <= max_length:\n            continue\n        else:\n            max_key = k\n            max_length = v\n    assert isinstance(max_length, int), 'The max length between all params must be an int'\n    # cycle through params for max length param, otherwise infinite cycle\n    values_padded = (cycle(v) if k!=max_key else v for k,v in params.items())\n    values_padded = zip(*values_padded)  # ('a', 1, 14), ('b', 2, 16), ('c', 3, 16) ...\n    values_padded = zip(*values_padded)  # ('a', 'b', 'c'), (1, 2, 3), (14, 15, 16)...\n    padded_params = {}\n    for k, v in zip(params, values_padded):\n        padded_params[k] = v\n    return padded_params\n    \npadded_params = _params_pad(rf_params)\n#rf_params\n#padded_params\n\n# Params sampler for individual GA\ndef _gen_params(padded_params=padded_params):\n    \"\"\"Get some params idx for individual\"\"\"\n    max_dict = len(padded_params)\n    max_length = len(list(padded_params.values())[0])\n    return [randint(0, max_length-1) for _ in range(max_dict)]\n\n#some_params = _gen_params()\n#some_params\n\nfrom deap import creator, base, algorithms as ga_algo, tools, cma\n\n# GA Fitness\ncreator.create('Fitness', base.Fitness, weights=(1, -1, -1))\ncreator.create('Individual', list, fitness=creator.Fitness)\n\n# GA toolbox\ntb = base.Toolbox()\n\n# Multiprocessing for GA\n### TODO code for n_jobs\nfrom multiprocessing import Pool\npool = Pool()\ntb.register(\"map\", pool.map)\n\n# GA Individual and Pop\ntb.register(\"individual\", tools.initIterate, creator.Individual, _gen_params)\n#tb.individual()\ntb.register(\"population\", tools.initRepeat, list, tb.individual)\n#tb.population(3)\n\n# Specific fitness\ndef eval_indiv(individual, padded_params, est, X_train, X_test, y_train, y_test, score):\n    \"\"\"Evaluate individual's genes (estimator's params)\"\"\"\n    # convert indiv genes to estimator params\n    indiv_params = {k : list(v)[idx] for (k,v), idx in zip(padded_params.items(), individual)}\n    ### TODO code for n_jobs\n    est.set_params(**{**indiv_params, **{'n_jobs': -1} })\n    est.fit(X_train, y_train)\n    # ALWAYS return tuple\n    obj1 = score(y_test, est.predict(X_test))\n    obj2 = int(individual[0]) # n_estimators\n    obj3 = float(np.quantile(est.predict_proba(X_test).prod(axis=1), 0.5))\n    return (obj1, obj2, obj3)\n    \n# Scikit Estimator\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nfrom sklearn.metrics import accuracy_score\n\n# GA selection, crossover and mutation\ntb.register(\"evaluate\",\n            eval_indiv,\n            padded_params=padded_params,\n            est=rf,\n            X_train=X_train_transf,\n            X_test=X_test_transf, \n            y_train=y_train, \n            y_test=y_test, \n            score=accuracy_score)\ntb.register(\"select\", tools.selTournament, tournsize=2)\ntb.register(\"mate\", tools.cxUniform, indpb=0.5)\ntb.register(\"mutate\", tools.mutShuffleIndexes, indpb=0.1)\n\n#_eval(tb.individual(), padded_params, rf, X_train_transf, X_test_transf, y_train, y_test, accuracy_score)\n\ndef main(tb=tb, tools=tools, ga_algo=ga_algo, n_pop=50, n_gen=50):\n    pop = tb.population(n=n_pop)\n    hof = tools.HallOfFame(2)\n    \n    # Stats stdout\n    stats1 = tools.Statistics(lambda ind: ind.fitness.values[0] )\n    stats2 = tools.Statistics(lambda ind: ind.fitness.values[1] )\n    stats3 = tools.Statistics(lambda ind: ind.fitness.values[2] )\n    stats = tools.MultiStatistics(acc=stats1, n_est=stats2, risk=stats3)\n    stats.register(\"avg\", np.mean)\n    #stats.register(\"std\", np.std)\n    #stats.register(\"min\", np.min)\n    #stats.register(\"max\", np.max)\n    \n    # History\n    #hist = tools.History()\n    #toolbox.decorate(\"select\", hist.decorator)\n    #tb.decorate(\"mate\", hist.decorator)\n    #tb.decorate(\"mutate\", hist.decorator)\n    #hist.update(pop)\n    \n    # GA Run\n    pop, log = ga_algo.eaSimple(pop, tb, cxpb=0.5, mutpb=0.1, ngen=n_gen, stats=stats, halloffame=hof, verbose=True)\n    \n    return pop, log, hof\n    \npop, log, hof = main()\n\nf'Best Params {hof[0]}'\nf'Best Params 2nd {hof[1]}'","metadata":{"execution":{"iopub.status.busy":"2021-12-26T18:02:18.206498Z","iopub.execute_input":"2021-12-26T18:02:18.206823Z","iopub.status.idle":"2021-12-26T18:03:21.422129Z","shell.execute_reply.started":"2021-12-26T18:02:18.206784Z","shell.execute_reply":"2021-12-26T18:03:21.421308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}