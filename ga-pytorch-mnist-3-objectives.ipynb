{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%reset -sf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-28T19:37:18.910402Z","iopub.execute_input":"2021-12-28T19:37:18.910781Z","iopub.status.idle":"2021-12-28T19:37:19.030367Z","shell.execute_reply.started":"2021-12-28T19:37:18.910752Z","shell.execute_reply":"2021-12-28T19:37:19.029664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2021-12-29T05:24:49.124972Z","iopub.execute_input":"2021-12-29T05:24:49.125535Z","iopub.status.idle":"2021-12-29T05:24:49.162216Z","shell.execute_reply.started":"2021-12-29T05:24:49.125445Z","shell.execute_reply":"2021-12-29T05:24:49.161263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install graphviz\n# !pip install torchviz","metadata":{"execution":{"iopub.status.busy":"2021-12-29T05:24:51.848084Z","iopub.execute_input":"2021-12-29T05:24:51.848618Z","iopub.status.idle":"2021-12-29T05:24:51.851356Z","shell.execute_reply.started":"2021-12-29T05:24:51.848585Z","shell.execute_reply":"2021-12-29T05:24:51.85074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pprint import pprint\nfrom collections import defaultdict, OrderedDict\nfrom random import choice, seed, randint\nfrom tqdm import tqdm\nfrom itertools import repeat, product, combinations, cycle\nfrom pathlib import Path\nfrom string import ascii_lowercase\n\nfrom torch import optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch import (load, amax as pt_amax, max as pt_max, ones, save, no_grad, stack, numel, tensor, \n                   manual_seed, sigmoid, tanh, add, mul, sub, div, amin as pt_amin, cat,\n                  maximum, minimum, device, cuda, rand, prod, median, log as pt_log, round as pt_round,\n                  isnan)\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\n#from torchviz import make_dot\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n\nfrom deap import creator as ga_cr, base as ga_b, algorithms as ga_algo, tools as ga_t","metadata":{"execution":{"iopub.status.busy":"2021-12-29T05:24:52.906272Z","iopub.execute_input":"2021-12-29T05:24:52.906586Z","iopub.status.idle":"2021-12-29T05:24:55.292214Z","shell.execute_reply.started":"2021-12-29T05:24:52.906536Z","shell.execute_reply":"2021-12-29T05:24:55.291279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.set_printoptions(suppress=True)\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","metadata":{"execution":{"iopub.status.busy":"2021-12-29T05:24:55.293862Z","iopub.execute_input":"2021-12-29T05:24:55.294095Z","iopub.status.idle":"2021-12-29T05:24:55.298324Z","shell.execute_reply.started":"2021-12-29T05:24:55.294066Z","shell.execute_reply":"2021-12-29T05:24:55.297708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = device(\"cuda\" if cuda.is_available() else \"cpu\")\n# device = \"cpu\"\nprint(\"==> Device:\", device)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T05:24:55.299376Z","iopub.execute_input":"2021-12-29T05:24:55.299723Z","iopub.status.idle":"2021-12-29T05:24:55.313644Z","shell.execute_reply.started":"2021-12-29T05:24:55.299693Z","shell.execute_reply":"2021-12-29T05:24:55.312782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DS(Dataset):\n    def __init__(self, maps, labels) -> None:\n        self.maps = maps\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        X = self.maps[idx]\n        #X = X.reshape(1, -1)\n        X = X.unsqueeze(0)\n        y = self.labels[idx]\n        return X.float(), y.long()\n\n# Data\nX_train, y_train = load('/kaggle/input/pytorch-mnist/training.pt')\nX_test, y_test = load('/kaggle/input/pytorch-mnist/test.pt')\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2021-12-29T05:24:55.315223Z","iopub.execute_input":"2021-12-29T05:24:55.315587Z","iopub.status.idle":"2021-12-29T05:24:56.17391Z","shell.execute_reply.started":"2021-12-29T05:24:55.315557Z","shell.execute_reply":"2021-12-29T05:24:56.173102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NET(nn.Module):\n    def __init__(self, \n                 batch_size,\n                 params={\n                    'feature_maps': 2, \n                    'f_kernel_size': 2, \n                    'f_stride_size': 2, \n                    'pools': 'MaxPool2d', \n                    'p_kernel_size': 2, \n                    'p_stride_size': 2, \n                    'activs': 'ELU'},\n                 outputs_dim=10,\n                ):\n        super().__init__()\n        \n        self.f_maps = params['feature_maps']\n        self.f_k_size = params['f_kernel_size']\n        self.f_s_size = params['f_stride_size']\n        self.pool = params['pools']\n        self.p_k_size = params['p_kernel_size']\n        self.p_s_size = params['p_stride_size']\n        self.activ = params['activs']\n        self.outputs_dim = outputs_dim  # MNIST\n        self.batch_size = batch_size\n        \n        self.model = nn.Sequential(\n            nn.Conv2d(1, \n                      self.f_maps, \n                      self.f_k_size, \n                      stride=self.f_s_size),\n            nn.Dropout2d(p=0.5),\n            nn.__getattribute__(self.pool)(kernel_size=self.p_k_size, \n                                           stride=self.p_s_size),\n            nn.__getattribute__(self.activ)())\n        \n        self.model_out_shape = tensor(self.model(ones(self.batch_size,1,28,28)).shape)\n        self.model_out_shape = int(prod(self.model_out_shape)/self.batch_size)\n        self.output = nn.Linear(self.model_out_shape, self.outputs_dim)\n                \n    def forward(self, x):\n        \"\"\"Forward\"\"\"\n        res = self.model(x)\n        res = res.reshape(self.batch_size, self.model_out_shape)\n        res = F.softmax(self.output(res), dim=1)\n        return res\n    \n    def count_weights_biases(self):\n        return int(sum(p.numel() for p in self.parameters() if p.requires_grad))\n    \n#net = NET(BATCH_SIZE).train()\n#num = np.random.randint(0, len(X_train))\n#_ = plt.imshow(X_train[num])\n#f'single image:, {X_train[num].shape}'\n#f'out: {net(ones(BATCH_SIZE, 1, 28, 28).float()).shape}'","metadata":{"execution":{"iopub.status.busy":"2021-12-29T05:30:49.978634Z","iopub.execute_input":"2021-12-29T05:30:49.978946Z","iopub.status.idle":"2021-12-29T05:30:49.992277Z","shell.execute_reply.started":"2021-12-29T05:30:49.978911Z","shell.execute_reply":"2021-12-29T05:30:49.991456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GA_Pytorch():\n    def __init__(self, \n                 params, \n                 eval_func,\n                 eval_weights,\n                 X_train,\n                 X_test,\n                 y_train,\n                 y_test,\n                 batch_size=64,\n                 lr=0.0001,\n                 sel_tournsize=2, \n                 cx_uniform_prob=0.5, \n                 mut_shuffle_idx_prob=0.1, \n                 n_pop=50, \n                 n_gen=20, \n                 n_hof=5, \n                 cx_prob=0.5, \n                 mut_prob=0.1, \n                 n_jobs=1\n                ):\n        self.params = params\n        self.eval_func = eval_func\n        self.eval_weights = eval_weights\n        \n        self.X_train = X_train\n        self.X_test = X_test\n        self.y_train = y_train\n        self.y_test = y_test\n        self.batch_size = batch_size\n        self.lr = lr\n        \n        self.sel_tournsize = sel_tournsize\n        self.cx_uniform_prob = cx_uniform_prob\n        self.mut_shuffle_idx_prob = mut_shuffle_idx_prob\n        self.n_pop = n_pop\n        self.n_gen = n_gen\n        self.n_hof = n_hof\n        self.cx_prob = cx_prob\n        self.mut_prob = mut_prob\n        \n        self.n_jobs = n_jobs\n\n        self._pad_params()\n        self._create_fitness_and_indiv()\n        self._register_indiv_and_pop_generators()\n        self._register_eval_func()\n        self._register_selection_crossover_mutation_methods()\n\n    def _pad_params(self):\n        \"\"\"Pad params for crossover shuffle idx method\"\"\"\n        assert isinstance(self.params, dict), 'Params must be a dict, i.e. estimator.get_params()'\n        params_count = {k: len(v) for k,v in self.params.items()}\n        max_length, max_key = -99, ''\n        for k, v in params_count.items():\n            if v <= max_length:\n                continue\n            else:\n                max_key = k\n                max_length = v\n        assert isinstance(max_length, int), 'The max length between all params must be an int'\n        # cycle through params for max length param, otherwise infinite cycle\n        values_padded = (cycle(v) if k!=max_key else v for k,v in self.params.items())\n        values_padded = zip(*values_padded)  # ('a', 1, 14), ('b', 2, 16), ('c', 3, 16) ...\n        values_padded = zip(*values_padded)  # ('a', 'b', 'c'), (1, 2, 3), (14, 15, 16)...\n        padded_params = {}\n        for k, v in zip(self.params, values_padded):\n            padded_params[k] = v\n        self.padded_params = padded_params\n        print('Params padded')\n\n    def _create_fitness_and_indiv(self):\n        \"\"\"Create GA individual and fitness entities (classes)\"\"\"\n        ga_cr.create('Fitness', ga_b.Fitness, weights=self.eval_weights)\n        ga_cr.create('Individual', list, fitness=ga_cr.Fitness)\n        print('GA entities created')\n\n    def _gen_params_to_ga(self):\n        \"\"\"Generate index for each param for individual\"\"\"\n        max_dict = len(self.padded_params)\n        max_length = len(list(self.padded_params.values())[0])\n        idxs = [randint(0, max_length-1) for _ in range(max_dict)]\n        return idxs\n    \n    def _register_indiv_and_pop_generators(self):\n        \"\"\"Register GA individual and population generators\"\"\"\n        self.tb = ga_b.Toolbox()\n\n        if self.n_jobs > 1:\n            from multiprocessing import Pool\n            pool = Pool()\n            self.tb.register(\"map\", pool.map)\n\n        self.tb.register(\"individual\", ga_t.initIterate, ga_cr.Individual, self._gen_params_to_ga)\n        self.tb.register(\"population\", ga_t.initRepeat, list, self.tb.individual)\n        print('GA entities\\' methods registered')\n        \n    def _register_eval_func(self):\n        \"\"\"Set GA evaluate individual function\"\"\"\n        self.tb.register(\"evaluate\",\n                        self.eval_func,\n                        padded_params=self.padded_params,\n                        X_train=self.X_train,\n                        X_test=self.X_test, \n                        y_train=self.y_train, \n                        y_test=self.y_test,\n                        batch_size=self.batch_size,\n                        lr=self.lr)\n        #print(list(self.tb.evaluate(indiv) for indiv in self.tb.population(3)))\n        print('GA eval function registered')\n    \n    def _register_selection_crossover_mutation_methods(self):\n        self.tb.register(\"select\", ga_t.selTournament, tournsize=self.sel_tournsize)\n        self.tb.register(\"mate\", ga_t.cxUniform, indpb=self.cx_uniform_prob)\n        self.tb.register(\"mutate\", ga_t.mutShuffleIndexes, indpb=self.mut_shuffle_idx_prob)\n        print('GA sel-cx-mut methods registered')\n        \n    def run_ga_search(self):\n        \"\"\"GA Search\"\"\"\n        pop = self.tb.population(n=self.n_pop)\n        hof = ga_t.HallOfFame(self.n_hof)\n\n        # Stats stdout\n        #stats = ga_t.Statistics(lambda ind: ind.fitness.values )\n        stats1 = ga_t.Statistics(lambda ind: ind.fitness.values[0] )\n        stats2 = ga_t.Statistics(lambda ind: ind.fitness.values[1] )\n        stats3 = ga_t.Statistics(lambda ind: ind.fitness.values[2] )\n        stats = ga_t.MultiStatistics(accuracy=stats1, risk=stats2, complexity=stats3)\n        stats.register(\"avg\", np.mean)\n        #stats.register(\"std\", np.std)\n        #stats.register(\"min\", np.min)\n        #stats.register(\"max\", np.max)\n\n        # History\n        #hist = tools.History()\n        #toolbox.decorate(\"select\", hist.decorator)\n        #tb.decorate(\"mate\", hist.decorator)\n        #tb.decorate(\"mutate\", hist.decorator)\n        #hist.update(pop)\n\n        # GA Run\n        pop, log = ga_algo.eaSimple(pop, self.tb, cxpb=self.cx_prob, \n                                    mutpb=self.mut_prob, ngen=self.n_gen, \n                                    stats=stats, halloffame=hof, verbose=True)\n        \n        # Convert back params\n        hof_ = {}\n        for i in range(self.n_hof):\n            hof_['hof_' + str(i)] = self._ga_to_params(hof[i])\n\n        return pop, log, hof_\n    \n    def _ga_to_params(self, idx_params):\n        \"\"\"Convert back idx to params\"\"\"\n        res = {}\n        for (k,v), idx in zip(self.padded_params.items(), idx_params):\n            res[k] = v[idx]\n        return res","metadata":{"execution":{"iopub.status.busy":"2021-12-29T05:30:50.348318Z","iopub.execute_input":"2021-12-29T05:30:50.348786Z","iopub.status.idle":"2021-12-29T05:30:50.380012Z","shell.execute_reply.started":"2021-12-29T05:30:50.348744Z","shell.execute_reply":"2021-12-29T05:30:50.379342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net_params = {\n    'feature_maps': np.linspace(1, 10, 10).astype(int),\n    'f_kernel_size': np.linspace(1, 4, 4).astype(int),\n    'f_stride_size': np.linspace(1, 4, 4).astype(int),\n    #\n    'pools': ['MaxPool2d', 'AvgPool2d'],\n    'p_kernel_size': np.linspace(1, 4, 4).astype(int),\n    'p_stride_size': np.linspace(1, 4, 4).astype(int),\n    #\n    'activs': ['ReLU', 'CELU', 'SELU', 'ELU', 'Softsign'],\n    #'norms': ['']\n}\n\ndef net_eval_indiv(individual, padded_params, X_train, X_test, y_train, y_test, batch_size, lr):\n    \"\"\"Evaluate individual's genes (estimator's params)\"\"\"\n\n    # Params\n    indiv_params = {k : list(v)[idx] for (k,v), idx in zip(padded_params.items(), individual)}\n    \n    # Net\n    net = NET(batch_size, params=indiv_params)\n    \n    # Optimizer\n    optimizer = Adam(net.parameters(), lr=lr)\n    criterion = nn.NLLLoss()\n    \n    # Train\n    train_ds = DS(X_train, y_train)  # TODO refactor out\n    train_dl = DataLoader(train_ds,\n                        batch_size=batch_size,\n                        shuffle=True,\n                        drop_last=True)\n    \n    for epoch in range(1):\n        #running_loss = []\n        train_correct = 0\n        train_total = 0\n        for i, (inputs, labels) in enumerate(train_dl):\n            if i <= 200:\n                #inputs = inputs.cuda()\n                #labels = labels.cuda()\n                outputs = net(inputs)\n                outputs = pt_log(outputs)\n\n                optimizer.zero_grad()\n                loss = criterion(outputs, labels).mean()\n                loss.backward()\n                optimizer.step()\n\n                # print statistics\n                #running_loss.append(loss.item())\n                _, predicted = pt_max(outputs.data, 1)\n                train_total += labels.size(0)\n                train_correct += (predicted == labels).sum().item()\n            else:\n                break\n        #print(f'TRAIN {train_correct / train_total * 100:^5.2f} %', end=' ')\n        \n    # Eval\n    with no_grad():\n        net = net.eval()\n        test_ds = DS(X_test, y_test)  # TODO refactor out\n        test_dl = DataLoader(test_ds,\n                            batch_size=batch_size,\n                            shuffle=True,\n                            drop_last=True)\n        #running_loss = []\n        test_correct = 0\n        test_total = 0\n        for i, (inputs, labels) in enumerate(test_dl):\n            if i <= 100:\n                #inputs = inputs.cuda()\n                #labels = labels.cuda()\n                outputs = net(inputs)\n\n                # print statistics\n                #running_loss.append(loss.item())\n                _, predicted = pt_max(outputs.data, 1)\n                test_total += labels.size(0)\n                test_correct += (predicted == labels).sum().item()\n                test_accuracy = test_correct / test_total * 100\n            else:\n                break\n        #print(f'TEST {test_accuracy:^5.2f} %')\n        \n    # Risk\n    risk = median(prod(net(inputs)*10, dim=1))\n    if isnan(risk):\n        risk = 10\n    else:\n        risk = float(risk)\n        \n    # Complexity\n    compl = net.count_weights_biases()\n\n    return (test_accuracy, risk, compl,)\n\nnet_weights = (1, -1, -1)","metadata":{"execution":{"iopub.status.busy":"2021-12-29T05:30:50.747894Z","iopub.execute_input":"2021-12-29T05:30:50.748166Z","iopub.status.idle":"2021-12-29T05:30:50.767688Z","shell.execute_reply.started":"2021-12-29T05:30:50.748138Z","shell.execute_reply":"2021-12-29T05:30:50.76684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"net_ga_params = GA_Pytorch(net_params, \n                           net_eval_indiv, \n                           net_weights,\n                           X_train, \n                           X_test, \n                           y_train, \n                           y_test)\npop, log, hof = net_ga_params.run_ga_search()","metadata":{"execution":{"iopub.status.busy":"2021-12-29T05:30:53.085596Z","iopub.execute_input":"2021-12-29T05:30:53.085891Z","iopub.status.idle":"2021-12-29T05:42:39.621316Z","shell.execute_reply.started":"2021-12-29T05:30:53.085861Z","shell.execute_reply":"2021-12-29T05:42:39.620644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(list(hof.values()))","metadata":{"execution":{"iopub.status.busy":"2021-12-29T06:15:40.710881Z","iopub.execute_input":"2021-12-29T06:15:40.711393Z","iopub.status.idle":"2021-12-29T06:15:40.801592Z","shell.execute_reply.started":"2021-12-29T06:15:40.7113Z","shell.execute_reply":"2021-12-29T06:15:40.800501Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Backup\n\nclass ARCH_NET(nn.Module):\n    def __init__(\n        self,\n        input_size=784,\n        hidden_dim=10,\n        output_dim=10,\n        nodes=5\n    ):\n        super().__init__()\n        \n        self.input_size = input_size\n        self.hidden_dim = hidden_dim\n        self.nodes = nodes\n        self.activs = [F.selu, F.relu, F.celu, F.elu, sigmoid, F.logsigmoid, F.softplus, F.softsign, tanh]\n        #self.activs = [F.elu, sigmoid, F.softsign]\n        #self.activs = [F.elu]\n        self.opers = [add, sub, mul, maximum, minimum]\n        #self.opers = [add]\n\n        self.abc = [letter*j for j in range(1, 99) for letter in ascii_lowercase]\n\n        self.init_layers = nn.ModuleDict()  # required for optim\n        self.init_activs = OrderedDict()\n\n        self.pairs = OrderedDict()\n        self.hidden_layers = nn.ModuleDict()\n        self.hidden_activs = OrderedDict()\n        self.hidden_activs_str = OrderedDict()\n        self.hidden_opers = OrderedDict()\n        self.hidden_opers_str = OrderedDict()\n\n        self.to_output = nn.Linear(hidden_dim, output_dim)\n        \n    def create_initial_layer(self):\n        \"\"\"Creates the NN initial layer\"\"\"\n        for i in range(2):\n            self.init_layers['i' + str(i)] = nn.Linear(self.input_size, self.hidden_dim)\n            self.init_activs['i' + str(i)] = choice(self.activs)\n\n    def create_node(self, options):\n        \"\"\"Create a NN node\"\"\"\n        pair = list(combinations(options, 2))\n        pair = choice(pair)\n \n        if pair in self.pairs:\n            self.create_node(options)\n        else:\n            self.options.append(pair)\n\n            self.hidden_opers[self.abc[len(self.hidden_layers)]] = choice(self.opers)\n            self.hidden_opers_str[pair] = self.hidden_opers[self.abc[len(self.hidden_layers)]].__name__\n            \n            self.pairs[pair] = self.abc[len(self.hidden_layers)]\n            self.hidden_layers[self.abc[len(self.hidden_layers)]] = nn.Linear(self.hidden_dim, self.hidden_dim)\n            \n            self.hidden_activs[self.abc[len(self.hidden_layers)]] = choice(self.activs)\n            self.hidden_activs_str[pair] = self.hidden_activs[self.abc[len(self.hidden_layers)]].__name__\n\n        return pair\n            \n    def create_random_arch(self):\n        \"\"\"Create a random NN Architecture\"\"\"\n        self.create_initial_layer()\n        for _ in range(self.nodes):\n            self.options = list(self.init_layers.keys())\n            self.options.extend(list(self.pairs.keys()))\n            self.create_node(self.options)\n        # print('Net Architecture is')\n        # pprint(self.pairs)\n\n        # def create_arch_from(self, arch):\n        # \"\"\"Creates a user-supplied NN Architecture\"\"\"\n        \n    def forward(self, x):\n        \"\"\"Forward/Predict\"\"\"\n        \n        # Pass inputs through initial layers...\n        init_l = {}\n        for (lay_k, lay), (act) in zip(\n            self.init_layers.items(),\n            self.init_activs.values()):\n            init_l[lay_k] = act(F.dropout(lay(x), p=0.2))\n        \n        # ...then, through hidden layers\n        hidden_l = {}\n        for (pair), (oper), (lay), (act) in zip(\n                self.pairs.keys(),\n                self.hidden_opers.values(),\n                self.hidden_layers.values(),\n                self.hidden_activs.values()\n        ):\n            hidden_l[pair] = oper( {**init_l, **hidden_l}[pair[0]], {**init_l, **hidden_l}[pair[1]] )\n            hidden_l[pair] = act(F.dropout(lay(hidden_l[pair]), p=0.2))\n        \n        # Check ending layers for output gather\n        end_layers, end_layers_str = [], []\n        for lay_k, lay_v in reversed(list(hidden_l.items())):\n            #print(end_layers_str)\n            if any([lay for lay in end_layers_str if str(lay_k) in lay]):\n                # if layer inside any previous layer...\n                #print('layer found inside other (continue)', str(lay_k))\n                continue\n            else:\n                # if not means that this is an ending layer for outputs\n                #print('layer not found inside others (end layer), save:', str(lay_k))\n                end_layers_str.append(str(lay_k))\n                end_layers.append(lay_v)\n        \n        # Outputs\n        end_layers = stack(end_layers, dim=0)\n        end_layers = pt_amax(end_layers, dim=0)\n        end_layers = self.to_output(end_layers)\n        end_layers = F.log_softmax(end_layers, dim=-1).squeeze()\n        return end_layers\n    \n#net = ARCH_NET(hidden_dim=64, nodes=5).train()\n#net.create_random_arch()\n#r = net(ones(1,784))","metadata":{"execution":{"iopub.status.busy":"2021-12-28T07:03:54.478233Z","iopub.status.idle":"2021-12-28T07:03:54.478992Z","shell.execute_reply.started":"2021-12-28T07:03:54.478786Z","shell.execute_reply":"2021-12-28T07:03:54.478808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}